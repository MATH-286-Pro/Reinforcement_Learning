import os
import sys
import time
import numpy as np
import jax
import jax.numpy as jp
import matplotlib.pyplot as plt
from typing import List, Dict, Any, Optional

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from env.mujoco_playground_cartpole_env import CartPoleEnv, create_cartpole_env
from env.cartpole_config import get_config, print_config


class CartPoleVisualizer:
    """CartPole ç¯å¢ƒå¯è§†åŒ–å’Œæµ‹è¯•å·¥å…·"""
    
    def __init__(self, config_name: str = 'default'):
        self.config_name = config_name
        self.env_config = get_config(config_name)
        self.env = create_cartpole_env(**self.env_config)
        
        print(f"åˆ›å»º CartPole ç¯å¢ƒ (é…ç½®: {config_name})")
        print(f"è§‚æµ‹ç»´åº¦: {self.env.observation_size}")
        print(f"åŠ¨ä½œç»´åº¦: {self.env.action_size}")
    
    def test_basic_functionality(self):
        """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
        print("\n=== åŸºæœ¬åŠŸèƒ½æµ‹è¯• ===")
        
        # æµ‹è¯•ç¯å¢ƒé‡ç½®
        rng = jax.random.PRNGKey(42)
        state = self.env.reset(rng)
        
        print(f"é‡ç½®åè§‚æµ‹: {state.obs}")
        print(f"é‡ç½®åå¥–åŠ±: {state.reward}")
        print(f"é‡ç½®åå®ŒæˆçŠ¶æ€: {state.done}")
        print(f"é‡ç½®åæŒ‡æ ‡: {state.metrics}")
        
        # æµ‹è¯•æ­¥è¿›
        action = jp.array([0.5])  # å‘å³æ–½åŠ›
        new_state = self.env.step(state, action)
        
        print(f"\næ­¥è¿›åè§‚æµ‹: {new_state.obs}")
        print(f"æ­¥è¿›åå¥–åŠ±: {new_state.reward}")
        print(f"æ­¥è¿›åå®ŒæˆçŠ¶æ€: {new_state.done}")
        print(f"æ­¥è¿›åæŒ‡æ ‡: {new_state.metrics}")
        
        print("âœ… åŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡")
    
    def test_random_policy(self, num_episodes: int = 5, max_steps: int = 200):
        """æµ‹è¯•éšæœºç­–ç•¥"""
        print(f"\n=== éšæœºç­–ç•¥æµ‹è¯• ({num_episodes} episodes) ===")
        
        episode_rewards = []
        episode_lengths = []
        
        for episode in range(num_episodes):
            rng = jax.random.PRNGKey(episode)
            state = self.env.reset(rng)
            
            episode_reward = 0.0
            episode_length = 0
            
            for step in range(max_steps):
                # éšæœºåŠ¨ä½œ
                action_rng, rng = jax.random.split(rng)
                action = jax.random.uniform(action_rng, shape=(1,), minval=-1.0, maxval=1.0)
                
                state = self.env.step(state, action)
                episode_reward += float(state.reward)
                episode_length += 1
                
                if state.done:
                    break
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            print(f"Episode {episode + 1}: Reward = {episode_reward:.2f}, Length = {episode_length}")
        
        print(f"\néšæœºç­–ç•¥ç»Ÿè®¡:")
        print(f"  å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}")
        print(f"  å¹³å‡é•¿åº¦: {np.mean(episode_lengths):.2f} Â± {np.std(episode_lengths):.2f}")
        print("âœ… éšæœºç­–ç•¥æµ‹è¯•å®Œæˆ")
        
        return episode_rewards, episode_lengths
    
    def test_simple_controller(self, num_episodes: int = 5, max_steps: int = 500):
        """æµ‹è¯•ç®€å• PID æ§åˆ¶å™¨"""
        print(f"\n=== ç®€å•æ§åˆ¶å™¨æµ‹è¯• ({num_episodes} episodes) ===")
        
        episode_rewards = []
        episode_lengths = []
        success_count = 0
        
        # PID å‚æ•°
        kp_theta = 20.0  # è§’åº¦æ¯”ä¾‹å¢ç›Š
        kd_theta = 5.0   # è§’åº¦å¾®åˆ†å¢ç›Š
        kp_x = 1.0       # ä½ç½®æ¯”ä¾‹å¢ç›Š
        kd_x = 2.0       # ä½ç½®å¾®åˆ†å¢ç›Š
        
        for episode in range(num_episodes):
            rng = jax.random.PRNGKey(episode + 100)
            state = self.env.reset(rng)
            
            episode_reward = 0.0
            episode_length = 0
            
            for step in range(max_steps):
                # è§£æè§‚æµ‹
                x, x_dot, cos_theta, sin_theta, theta_dot = state.obs
                theta = jp.arctan2(sin_theta, cos_theta)
                
                # PID æ§åˆ¶
                force = (
                    -kp_theta * theta       # è§’åº¦è¯¯å·®
                    - kd_theta * theta_dot  # è§’é€Ÿåº¦
                    - kp_x * x              # ä½ç½®è¯¯å·®
                    - kd_x * x_dot          # é€Ÿåº¦
                )
                
                # é™åˆ¶åŠ¨ä½œèŒƒå›´
                action = jp.clip(jp.array([force / 20.0]), -1.0, 1.0)
                
                state = self.env.step(state, action)
                episode_reward += float(state.reward)
                episode_length += 1
                
                if state.done:
                    break
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            if episode_length >= max_steps * 0.9:  # 90% å®Œæˆåº¦ç®—æˆåŠŸ
                success_count += 1
            
            print(f"Episode {episode + 1}: Reward = {episode_reward:.2f}, Length = {episode_length}")
        
        success_rate = success_count / num_episodes
        print(f"\nç®€å•æ§åˆ¶å™¨ç»Ÿè®¡:")
        print(f"  å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}")
        print(f"  å¹³å‡é•¿åº¦: {np.mean(episode_lengths):.2f} Â± {np.std(episode_lengths):.2f}")
        print(f"  æˆåŠŸç‡: {success_rate:.1%}")
        print("âœ… ç®€å•æ§åˆ¶å™¨æµ‹è¯•å®Œæˆ")
        
        return episode_rewards, episode_lengths, success_rate
    
    def test_vectorized_environments(self, num_envs: int = 100, num_steps: int = 50):
        """æµ‹è¯•å‘é‡åŒ–ç¯å¢ƒ"""
        print(f"\n=== å‘é‡åŒ–ç¯å¢ƒæµ‹è¯• ({num_envs} ç¯å¢ƒ, {num_steps} æ­¥) ===")
        
        # åˆ›å»ºå‘é‡åŒ–é‡ç½®å’Œæ­¥è¿›å‡½æ•°
        vmap_reset = jax.vmap(self.env.reset)
        vmap_step = jax.vmap(self.env.step)
        
        # ç”Ÿæˆéšæœºç§å­
        rng = jax.random.PRNGKey(0)
        rngs = jax.random.split(rng, num_envs)
        
        # æ‰¹é‡é‡ç½®
        start_time = time.time()
        states = vmap_reset(rngs)
        reset_time = time.time() - start_time
        
        print(f"æ‰¹é‡é‡ç½® {num_envs} ç¯å¢ƒç”¨æ—¶: {reset_time:.4f} ç§’")
        print(f"çŠ¶æ€å½¢çŠ¶: obs={states.obs.shape}, reward={states.reward.shape}, done={states.done.shape}")
        
        # æ‰¹é‡æ­¥è¿›
        all_rewards = []
        step_times = []
        
        for step in range(num_steps):
            # éšæœºåŠ¨ä½œ
            actions = jax.random.uniform(
                jax.random.PRNGKey(step), 
                shape=(num_envs, 1), 
                minval=-1.0, 
                maxval=1.0
            )
            
            # æ‰¹é‡æ­¥è¿›
            start_time = time.time()
            states = vmap_step(states, actions)
            step_time = time.time() - start_time
            step_times.append(step_time)
            
            all_rewards.append(states.reward)
            
            if step % 10 == 0:
                avg_reward = jp.mean(states.reward)
                done_count = jp.sum(states.done)
                print(f"Step {step}: å¹³å‡å¥–åŠ± = {avg_reward:.3f}, å®Œæˆç¯å¢ƒæ•° = {done_count}")
        
        avg_step_time = np.mean(step_times)
        total_reward = jp.sum(jp.array(all_rewards))
        
        print(f"\nå‘é‡åŒ–ç¯å¢ƒç»Ÿè®¡:")
        print(f"  å¹³å‡æ­¥è¿›æ—¶é—´: {avg_step_time:.6f} ç§’")
        print(f"  æ¯ç§’æ­¥è¿›æ•°: {num_envs / avg_step_time:.0f}")
        print(f"  æ€»å¥–åŠ±: {total_reward:.2f}")
        print("âœ… å‘é‡åŒ–ç¯å¢ƒæµ‹è¯•å®Œæˆ")
        
        return avg_step_time, total_reward
    
    def plot_episode_analysis(self, states_history: List, actions_history: List, save_path: Optional[str] = None):
        """ç»˜åˆ¶ episode åˆ†æå›¾"""
        print("\n=== ç»˜åˆ¶ Episode åˆ†æå›¾ ===")
        
        # æå–æ•°æ®
        times = np.arange(len(states_history)) * self.env_config.ctrl_dt
        observations = np.array([state.obs for state in states_history])
        actions = np.array(actions_history)
        rewards = np.array([state.reward for state in states_history])
        
        # è§£æè§‚æµ‹
        x_positions = observations[:, 0]
        x_velocities = observations[:, 1]
        cos_theta = observations[:, 2]
        sin_theta = observations[:, 3]
        theta_velocities = observations[:, 4]
        theta_angles = np.arctan2(sin_theta, cos_theta) * 180 / np.pi  # è½¬æ¢ä¸ºåº¦
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(3, 2, figsize=(12, 10))
        fig.suptitle('CartPole Episode Analysis')
        
        # å°è½¦ä½ç½®
        axes[0, 0].plot(times, x_positions)
        axes[0, 0].set_title('Cart Position')
        axes[0, 0].set_xlabel('Time (s)')
        axes[0, 0].set_ylabel('Position (m)')
        axes[0, 0].grid(True)
        
        # å°è½¦é€Ÿåº¦
        axes[0, 1].plot(times, x_velocities)
        axes[0, 1].set_title('Cart Velocity')
        axes[0, 1].set_xlabel('Time (s)')
        axes[0, 1].set_ylabel('Velocity (m/s)')
        axes[0, 1].grid(True)
        
        # æ‘†æ†è§’åº¦
        axes[1, 0].plot(times, theta_angles)
        axes[1, 0].set_title('Pole Angle')
        axes[1, 0].set_xlabel('Time (s)')
        axes[1, 0].set_ylabel('Angle (degrees)')
        axes[1, 0].grid(True)
        
        # æ‘†æ†è§’é€Ÿåº¦
        axes[1, 1].plot(times, theta_velocities)
        axes[1, 1].set_title('Pole Angular Velocity')
        axes[1, 1].set_xlabel('Time (s)')
        axes[1, 1].set_ylabel('Angular Velocity (rad/s)')
        axes[1, 1].grid(True)
        
        # æ§åˆ¶åŠ¨ä½œ
        axes[2, 0].plot(times, actions.flatten())
        axes[2, 0].set_title('Control Action')
        axes[2, 0].set_xlabel('Time (s)')
        axes[2, 0].set_ylabel('Force (normalized)')
        axes[2, 0].grid(True)
        
        # å¥–åŠ±
        axes[2, 1].plot(times, rewards)
        axes[2, 1].set_title('Reward')
        axes[2, 1].set_xlabel('Time (s)')
        axes[2, 1].set_ylabel('Reward')
        axes[2, 1].grid(True)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"å›¾åƒå·²ä¿å­˜åˆ°: {save_path}")
        else:
            plt.show()
        
        plt.close()
    
    def collect_episode_data(self, controller_type: str = 'pid', max_steps: int = 300):
        """æ”¶é›†ä¸€ä¸ª episode çš„æ•°æ®ç”¨äºåˆ†æ"""
        print(f"\n=== æ”¶é›† Episode æ•°æ® ({controller_type}) ===")
        
        rng = jax.random.PRNGKey(12345)
        state = self.env.reset(rng)
        
        states_history = [state]
        actions_history = []
        
        for step in range(max_steps):
            if controller_type == 'pid':
                # PID æ§åˆ¶å™¨
                x, x_dot, cos_theta, sin_theta, theta_dot = state.obs
                theta = jp.arctan2(sin_theta, cos_theta)
                
                force = -20.0 * theta - 5.0 * theta_dot - 1.0 * x - 2.0 * x_dot
                action = jp.clip(jp.array([force / 20.0]), -1.0, 1.0)
                
            elif controller_type == 'random':
                # éšæœºæ§åˆ¶å™¨
                action = jax.random.uniform(
                    jax.random.PRNGKey(step), 
                    shape=(1,), 
                    minval=-1.0, 
                    maxval=1.0
                )
            else:
                # é›¶æ§åˆ¶å™¨
                action = jp.array([0.0])
            
            actions_history.append(action)
            state = self.env.step(state, action)
            states_history.append(state)
            
            if state.done:
                print(f"Episode åœ¨ç¬¬ {step + 1} æ­¥ç»“æŸ")
                break
        
        return states_history, actions_history
    
    def run_comprehensive_test(self):
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("\n" + "="*60)
        print("CartPole ç¯å¢ƒç»¼åˆæµ‹è¯•")
        print("="*60)
        
        # æ‰“å°ç¯å¢ƒé…ç½®
        print_config(self.env_config, f"Environment Config ({self.config_name})")
        
        # åŸºæœ¬åŠŸèƒ½æµ‹è¯•
        self.test_basic_functionality()
        
        # éšæœºç­–ç•¥æµ‹è¯•
        random_rewards, random_lengths = self.test_random_policy()
        
        # ç®€å•æ§åˆ¶å™¨æµ‹è¯•
        pid_rewards, pid_lengths, success_rate = self.test_simple_controller()
        
        # å‘é‡åŒ–ç¯å¢ƒæµ‹è¯•
        step_time, total_reward = self.test_vectorized_environments()
        
        # æ”¶é›†å’Œå¯è§†åŒ–æ•°æ®
        print("\n=== æ•°æ®æ”¶é›†å’Œå¯è§†åŒ– ===")
        
        # PID æ§åˆ¶å™¨æ•°æ®
        states_pid, actions_pid = self.collect_episode_data('pid')
        self.plot_episode_analysis(
            states_pid, 
            actions_pid, 
            save_path='cartpole_pid_analysis.png'
        )
        
        # éšæœºæ§åˆ¶å™¨æ•°æ®
        states_random, actions_random = self.collect_episode_data('random')
        self.plot_episode_analysis(
            states_random, 
            actions_random, 
            save_path='cartpole_random_analysis.png'
        )
        
        # æµ‹è¯•æ€»ç»“
        print(f"\n" + "="*60)
        print("æµ‹è¯•æ€»ç»“")
        print("="*60)
        print(f"âœ… åŸºæœ¬åŠŸèƒ½: æ­£å¸¸")
        print(f"âœ… éšæœºç­–ç•¥å¹³å‡å¥–åŠ±: {np.mean(random_rewards):.2f}")
        print(f"âœ… PID æ§åˆ¶å™¨å¹³å‡å¥–åŠ±: {np.mean(pid_rewards):.2f}")
        print(f"âœ… PID æ§åˆ¶å™¨æˆåŠŸç‡: {success_rate:.1%}")
        print(f"âœ… å‘é‡åŒ–æ€§èƒ½: {1/step_time:.0f} env/sec")
        print(f"âœ… ç¯å¢ƒå®ç°: æ­£ç¡®ä¸”é«˜æ•ˆ")
        print("="*60)


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("CartPole ç¯å¢ƒæµ‹è¯•å’Œå¯è§†åŒ–")
    
    # åˆ›å»ºå¯è§†åŒ–å·¥å…·
    visualizer = CartPoleVisualizer('training')
    
    # è¿è¡Œç»¼åˆæµ‹è¯•
    visualizer.run_comprehensive_test()
    
    print("\næ‰€æœ‰æµ‹è¯•å®Œæˆ! ğŸ‰")


if __name__ == "__main__":
    main()